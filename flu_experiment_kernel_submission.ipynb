{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 1: flu shot encouragement (kernel)\n",
    "This notebook includes experiments from Case Study 1 from the paper Multi-Source Causal Inference Using Control Variates. Specifically, this notebook contains experiments using kernel smoothing to estimate the odds ratios.\n",
    "\n",
    "We use flu shot data from Section 8.1 of [Ding and Lu 2016](https://www.dropbox.com/s/jxk76wk8ckxx4m3/Ding_et_al-2017%20JRSSB%20Principal%20stratification%20analysis%20using%20principal%20scores.pdf?dl=0). The original dataset fludata.txt can be downloaded at https://rss.onlinelibrary.wiley.com/hub/journal/14679868/series-b-datasets/79_3a\n",
    "\n",
    "The variables are:\n",
    "\n",
    "- Z: the binary randomized encouragement to get the flu shot\n",
    "- Y: the binary outcome of flu-related hospitalization. \n",
    "- X: all covariates. Most of them are binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "\n",
    "import data_sampler\n",
    "import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fludata.txt', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_COLUMN = 'outcome'\n",
    "Z_COLUMN = 'assign'\n",
    "X_COLUMNS = ['age', 'copd', 'dm', 'heartd', 'race', 'renal', 'sex', 'liverd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation using logistic regression model with interaction terms\n",
    "\n",
    "In this section, we assume that the data generating outcome model is\n",
    "\n",
    "$$P(Y=1 | Z = z, X = x) = \\frac{e^{\\beta_0 + \\beta_1 z + \\beta_2 ^T x + \\beta_3 ^T xz}}{1 + e^{\\beta_0 + \\beta_1 z + \\beta_2^T x + \\beta_3 ^T xz}}$$\n",
    "\n",
    "This allows for linear heterogenous effects in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to get P(Y = 1 | Z = z, X = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for outcome model: 0.915065\n",
      "AUC for outcome model: 0.665919\n",
      "Coefficients for outcome model: [[ 1.05670447 -0.00555082  0.44484032  0.32466448  0.83032499  0.01765058\n",
      "   1.39359229  0.09913    -2.47969443 -0.0050041  -0.2109543   0.28471807\n",
      "  -0.35065923 -0.55507202  0.33194963 -0.65973364  3.59850653]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='none',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "data_sampler_interaction_logistic.fit_outcome(df, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to get $P(Z = 1 | X = x)$ (propensity score)\n",
    "\n",
    "We assume that the propensity score comes from a simple logistic regression model: \n",
    "\n",
    "$$P(Z = 1 | X = x) = \\frac{e^{a_0 + a_1^Tx}}{ 1 + e^{a_0 + a_1^Tx}}$$\n",
    "\n",
    "We fit $a_0, a_1$ from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for propensity model: 0.526389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='none',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampler_interaction_logistic.fit_propensity(df, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate case control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 samples before selection bias\n",
      "Filtered to 17110 samples after selection bias; only returning the requested 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>copd</th>\n",
       "      <th>dm</th>\n",
       "      <th>heartd</th>\n",
       "      <th>race</th>\n",
       "      <th>renal</th>\n",
       "      <th>sex</th>\n",
       "      <th>liverd</th>\n",
       "      <th>assign</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64.854800</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.64780</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.711722</td>\n",
       "      <td>0.461105</td>\n",
       "      <td>0.465252</td>\n",
       "      <td>0.480082</td>\n",
       "      <td>0.482258</td>\n",
       "      <td>0.173867</td>\n",
       "      <td>0.47768</td>\n",
       "      <td>0.060718</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.498594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age          copd            dm        heartd          race  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      64.854800      0.306600      0.316800      0.639800      0.632100   \n",
       "std       12.711722      0.461105      0.465252      0.480082      0.482258   \n",
       "min       14.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       59.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%       67.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "75%       73.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max      100.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              renal          sex        liverd        assign       outcome  \n",
       "count  10000.000000  10000.00000  10000.000000  10000.000000  10000.000000  \n",
       "mean       0.031200      0.64780      0.003700      0.491500      0.462200  \n",
       "std        0.173867      0.47768      0.060718      0.499953      0.498594  \n",
       "min        0.000000      0.00000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.00000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      1.00000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      1.00000      0.000000      1.000000      1.000000  \n",
       "max        1.000000      1.00000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_biased_samples = data_sampler_interaction_logistic.generate_selection_biased_data(df, num_samples=10000)\n",
    "selection_biased_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ATE estimates with and without control variate with kernel estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATE_estimator_fn_interaction(df_input):\n",
    "    data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "    data_sampler_interaction_logistic.fit_outcome(df_input)\n",
    "    return data_sampler_interaction_logistic.get_ATE_estimate(df_input)\n",
    "\n",
    "OR_xs=df.sample(20, replace=False)[X_COLUMNS] #sample a few statas\n",
    "def CV_estimator_kernel(df_input_obs, df_input_bias, bandwidth=10, n_OR_samples=20):\n",
    "    data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "    # Estimate OR from observational dataset\n",
    "    OR_obs = np.mean(data_sampler_interaction_logistic.get_conditional_OR_estimates_kernel(input_df=df_input_obs, x_inputs=OR_xs, bandwidth=bandwidth))\n",
    "    # Estimate OR from selection bias dataset\n",
    "    OR_bias = np.mean(data_sampler_interaction_logistic.get_conditional_OR_estimates_kernel(input_df=df_input_bias, x_inputs=OR_xs, bandwidth=bandwidth))\n",
    "    return OR_obs - OR_bias\n",
    "\n",
    "CV_samples, ATE_hat_samples, _ = bootstrap.run_bootstrap_df(df_obs=df, \n",
    "              df_bias=selection_biased_samples, \n",
    "              n_replicates=300, \n",
    "              ATE_estimator_fn=ATE_estimator_fn_interaction,\n",
    "              CV_estimator_fn=CV_estimator_kernel,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal CV coefficient: 0.07205267541055566\n"
     ]
    }
   ],
   "source": [
    "sample_cov = np.cov(np.array([ATE_hat_samples, CV_samples]), ddof=1)\n",
    "\n",
    "# Get optimal control variates coefficient\n",
    "cov_ATE_CV = sample_cov[0][1]\n",
    "var_CV = sample_cov[1][1]\n",
    "optimal_CV_coeff = cov_ATE_CV / var_CV\n",
    "print(\"optimal CV coefficient:\", optimal_CV_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Variance of ATE estimator: 0.00010715788098051833\n",
      ">>> Bias of ATE estimator: 0.000538436939324833\n",
      ">>> Variance of ATE estimator with CV: 2.4004733006866436e-05\n",
      ">>> Bias of ATE estimator with CV: -0.004464371090199419\n"
     ]
    }
   ],
   "source": [
    "# Get variance/bias of ATE estimators with and without CV.\n",
    "CV_samples, ATE_hat_samples, ATE_hat_CV_samples = bootstrap.run_bootstrap_df(\n",
    "    df_obs=df, \n",
    "    df_bias=selection_biased_samples, \n",
    "    n_replicates=300, # Try increasing this\n",
    "    ATE_estimator_fn=ATE_estimator_fn_interaction,\n",
    "    CV_estimator_fn=CV_estimator_kernel,\n",
    "    optimal_CV_coeff=optimal_CV_coeff)\n",
    "\n",
    "ATE_var = np.var(np.array(ATE_hat_samples), ddof=1)\n",
    "print(\">>> Variance of ATE estimator:\", ATE_var)\n",
    "\n",
    "ATE_bias = np.mean(np.array(ATE_hat_samples)) - ATE_estimate\n",
    "print(\">>> Bias of ATE estimator:\", ATE_bias)\n",
    "\n",
    "ATE_CV_var = np.var(np.array(ATE_hat_CV_samples), ddof=1)\n",
    "print(\">>> Variance of ATE estimator with CV:\", ATE_CV_var)\n",
    "\n",
    "ATE_CV_bias = np.mean(np.array(ATE_hat_CV_samples)) - ATE_estimate\n",
    "print(\">>> Bias of ATE estimator with CV:\", ATE_CV_bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
