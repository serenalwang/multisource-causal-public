{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 1: flu shot encouragement (logistic)\n",
    "This notebook includes experiments from Case Study 1 from the paper Multi-Source Causal Inference Using Control Variates. Specifically, this notebook contains experiments using the logistic model to estimate the ATE and odds ratios.\n",
    "\n",
    "We use flu shot data from Section 8.1 of [Ding and Lu 2016](https://www.dropbox.com/s/jxk76wk8ckxx4m3/Ding_et_al-2017%20JRSSB%20Principal%20stratification%20analysis%20using%20principal%20scores.pdf?dl=0). The original dataset fludata.txt can be downloaded at https://rss.onlinelibrary.wiley.com/hub/journal/14679868/series-b-datasets/79_3a\n",
    "\n",
    "The variables are:\n",
    "\n",
    "- Z: the binary randomized encouragement to get the flu shot\n",
    "- Y: the binary outcome of flu-related hospitalization. \n",
    "- X: all covariates. Most of them are binary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "\n",
    "import data_sampler\n",
    "import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fludata.txt', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_COLUMN = 'outcome'\n",
    "Z_COLUMN = 'assign'\n",
    "X_COLUMNS = ['age', 'copd', 'dm', 'heartd', 'race', 'renal', 'sex', 'liverd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation using logistic regression model with interaction terms\n",
    "\n",
    "In this section, we assume that the data generating outcome model is\n",
    "\n",
    "$$P(Y=1 | Z = z, X = x) = \\frac{e^{\\beta_0 + \\beta_1 z + \\beta_2 ^T x + \\beta_3 ^T xz}}{1 + e^{\\beta_0 + \\beta_1 z + \\beta_2^T x + \\beta_3 ^T xz}}$$\n",
    "\n",
    "This allows for linear heterogenous effects in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to get P(Y = 1 | Z = z, X = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for outcome model: 0.915065\n",
      "Training AUC for outcome model: 0.666010\n",
      "Coefficients for outcome model: [[ 9.68703784e-01 -6.16308889e-03  4.39076268e-01  3.34411150e-01\n",
      "   8.32099888e-01  1.30415691e-02  1.34967584e+00  9.08488042e-02\n",
      "  -3.71535058e+00 -3.91651689e-03 -2.02438589e-01  2.73969047e-01\n",
      "  -3.58542569e-01 -5.37481299e-01  3.67910944e-01 -6.42304872e-01\n",
      "   4.79688067e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='none',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "data_sampler_interaction_logistic.fit_outcome(df, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model to get $P(Z = 1 | X = x)$ (propensity score)\n",
    "\n",
    "We assume that the propensity score comes from a simple logistic regression model: \n",
    "\n",
    "$$P(Z = 1 | X = x) = \\frac{e^{a_0 + a_1^Tx}}{ 1 + e^{a_0 + a_1^Tx}}$$\n",
    "\n",
    "We fit $a_0, a_1$ from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for propensity model: 0.526389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='none',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampler_interaction_logistic.fit_propensity(df, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate case control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 samples before selection bias\n",
      "Filtered to 16788 samples after selection bias; only returning the requested 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>copd</th>\n",
       "      <th>dm</th>\n",
       "      <th>heartd</th>\n",
       "      <th>race</th>\n",
       "      <th>renal</th>\n",
       "      <th>sex</th>\n",
       "      <th>liverd</th>\n",
       "      <th>assign</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64.922900</td>\n",
       "      <td>0.31400</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.635800</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.636500</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.533318</td>\n",
       "      <td>0.46414</td>\n",
       "      <td>0.462228</td>\n",
       "      <td>0.481229</td>\n",
       "      <td>0.483731</td>\n",
       "      <td>0.167815</td>\n",
       "      <td>0.481031</td>\n",
       "      <td>0.057354</td>\n",
       "      <td>0.499972</td>\n",
       "      <td>0.498325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age         copd            dm        heartd          race  \\\n",
       "count  10000.000000  10000.00000  10000.000000  10000.000000  10000.000000   \n",
       "mean      64.922900      0.31400      0.309300      0.635800      0.626600   \n",
       "std       12.533318      0.46414      0.462228      0.481229      0.483731   \n",
       "min       14.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%       59.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%       67.000000      0.00000      0.000000      1.000000      1.000000   \n",
       "75%       73.000000      1.00000      1.000000      1.000000      1.000000   \n",
       "max      100.000000      1.00000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              renal           sex        liverd        assign       outcome  \n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
       "mean       0.029000      0.636500      0.003300      0.492700      0.458800  \n",
       "std        0.167815      0.481031      0.057354      0.499972      0.498325  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      1.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      1.000000      0.000000      1.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_biased_samples = data_sampler_interaction_logistic.generate_selection_biased_data(df, num_samples=10000)\n",
    "selection_biased_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ATE estimates with and without control variate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATE_estimator_fn_interaction(df_input):\n",
    "    data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "    data_sampler_interaction_logistic.fit_outcome(df_input)\n",
    "    return data_sampler_interaction_logistic.get_ATE_estimate(df_input)\n",
    "\n",
    "def CV_estimator_fn_interaction(df_input_obs, df_input_bias):\n",
    "    data_sampler_interaction_logistic = data_sampler.DataSamplerInteractionLogistic(Z_COLUMN, X_COLUMNS, Y_COLUMN)\n",
    "    OR_xs = df_input_obs[X_COLUMNS] # Average over all xs in the observational dataset.\n",
    "    # Estimate OR from observational dataset\n",
    "    data_sampler_interaction_logistic.fit_outcome(df_input_obs)\n",
    "    OR_obs = np.mean(data_sampler_interaction_logistic.get_conditional_OR_estimates(OR_xs))\n",
    "    # Estimate OR from selection bias dataset\n",
    "    data_sampler_interaction_logistic.fit_outcome(df_input_bias)\n",
    "    OR_bias = np.mean(data_sampler_interaction_logistic.get_conditional_OR_estimates(OR_xs))\n",
    "    return OR_obs - OR_bias\n",
    "\n",
    "CV_samples, ATE_hat_samples, _ = bootstrap.run_bootstrap_df(df_obs=df, \n",
    "              df_bias=selection_biased_samples, \n",
    "              n_replicates=300, \n",
    "              ATE_estimator_fn=ATE_estimator_fn_interaction,\n",
    "              CV_estimator_fn=CV_estimator_fn_interaction,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal CV coefficient: 0.057495495362146105\n"
     ]
    }
   ],
   "source": [
    "sample_cov = np.cov(np.array([ATE_hat_samples, CV_samples]), ddof=1)\n",
    "\n",
    "# Get optimal control variates coefficient\n",
    "cov_ATE_CV = sample_cov[0][1]\n",
    "var_CV = sample_cov[1][1]\n",
    "optimal_CV_coeff = cov_ATE_CV / var_CV\n",
    "print(\"optimal CV coefficient:\", optimal_CV_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Variance of ATE estimator: 0.000102341575760821\n",
      ">>> Bias of ATE estimator: 0.0004283211889470496\n",
      ">>> Variance of ATE estimator with CV: 2.9515768379399486e-05\n",
      ">>> Bias of ATE estimator with CV: 0.0006431299551389984\n"
     ]
    }
   ],
   "source": [
    "# Get variance/bias of ATE estimators with and without CV.\n",
    "CV_samples, ATE_hat_samples, ATE_hat_CV_samples = bootstrap.run_bootstrap_df(\n",
    "    df_obs=df, \n",
    "    df_bias=selection_biased_samples, \n",
    "    n_replicates=300, # Try increasing this\n",
    "    ATE_estimator_fn=ATE_estimator_fn_interaction,\n",
    "    CV_estimator_fn=CV_estimator_fn_interaction,\n",
    "    optimal_CV_coeff=optimal_CV_coeff)\n",
    "\n",
    "ATE_var = np.var(np.array(ATE_hat_samples), ddof=1)\n",
    "print(\">>> Variance of ATE estimator:\", ATE_var)\n",
    "\n",
    "ATE_bias = np.mean(np.array(ATE_hat_samples)) - ATE_estimate\n",
    "print(\">>> Bias of ATE estimator:\", ATE_bias)\n",
    "\n",
    "ATE_CV_var = np.var(np.array(ATE_hat_CV_samples), ddof=1)\n",
    "print(\">>> Variance of ATE estimator with CV:\", ATE_CV_var)\n",
    "\n",
    "ATE_CV_bias = np.mean(np.array(ATE_hat_CV_samples)) - ATE_estimate\n",
    "print(\">>> Bias of ATE estimator with CV:\", ATE_CV_bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
